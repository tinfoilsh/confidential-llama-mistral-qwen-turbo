shim-version: v0.1.8@sha256:795145f871b66946ab7755eb71fce866a92e4035a95d6f949ad79adee7294648
cvm-version: 0.4.1
cpus: 32
memory: 524288
gpus: full
vllm: false

models:
  - name: "llama3-3-70b-fp8"
    repo: "RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic@984c96b73bcf6a675945bac6382b9ed551e5d42b"
    mpk: "0d0be05062e4d3fadc17176cffd1ef6b518a02b0410bd897723f423074ec6cb5_72687374336_99f5f660-3ee0-5626-9772-2f082314e763"
  - name: "qwen2-5-72b"
    repo: "Qwen/Qwen2.5-72B-Instruct-AWQ@698703eae6604af048a3d2f509995dc302088217"
    mpk: "ddd9eec6024430f459b9e5e24591a995329e26cf63cfc7c3465fce17b081ba58_41607450624_d096fc1d-3279-5454-9fc0-cb0f2c8c7358"
  - name: "mistral-small-3-1-24b"
    repo: "mistralai/Mistral-Small-3.1-24B-Instruct-2503@4b8dd8aae705887db5295fcbff4aedbb92d682eb"
    mpk: "23cd0af7cb8f25d98be62011728497ffddcbefca748fe273c36bb823fd7bf95f_96077783040_e60a5d74-a8de-56df-a0b3-00f55e7a0d47"

containers:
  - name: "llama3-3-70b-fp8-tp4"
    image: ""
    args: [
      "--runtime", "nvidia",
      "--gpus", "'\"device=0,1,2,3\"'",
      "--ipc", "host",
      "vllm/vllm-openai:v0.9.2",
      "--model", "/tinfoil/mpk/mpk-0d0be05062e4d3fadc17176cffd1ef6b518a02b0410bd897723f423074ec6cb5",
      "--tensor-parallel-size", "4",
      "--served-model-name", "llama3-3-70b",
      "--port", "8001"
    ]
  - name: "qwen2-5-72b-int4-tp2"
    image: ""
    args: [
      "--runtime", "nvidia",
      "--gpus", "'\"device=4,5\"'",
      "--ipc", "host",
      "vllm/vllm-openai:v0.9.2",
      "--model", "/tinfoil/mpk/mpk-ddd9eec6024430f459b9e5e24591a995329e26cf63cfc7c3465fce17b081ba58",
      "--tensor-parallel-size", "2",
      "--served-model-name", "qwen2-5-72b-turbo",
      "--port", "8002"
    ]
  - name: "mistral3-1-24b-tp2"
    image: ""
    args: [
      "--runtime", "nvidia",
      "--gpus", "'\"device=6,7\"'",
      "--ipc", "host",
      "vllm/vllm-openai:v0.9.2",
      "--model", "/tinfoil/mpk/mpk-23cd0af7cb8f25d98be62011728497ffddcbefca748fe273c36bb823fd7bf95f",
      "--tensor-parallel-size", "2",
      "--served-model-name", "mistral-small-3-1-24b-turbo",
      "--port", "8003",
      "--gpu-memory-utilization", "0.95",
      "--limit_mm_per_prompt", "image=10"
    ]
  - name: "model-router"
    image: ""
    args: [
      "ghcr.io/tinfoilsh/model-router:0.0.1",
      "/app/bin", "-m", "llama3-3-70b_8001,qwen2-5-72b-turbo_8002,mistral-small-3-1-24b-turbo_8003",
    ]

shim:
  listen-port: 443
  upstream-port: 8087
  publish-attestation: false
  tls-challenge: dns
  control-plane: https://api.tinfoil.sh
  paths:
    - /v1/chat/completions
    - /metrics
  origins:
    - https://tinfoil.sh
    - https://chat.tinfoil.sh
    - http://localhost:3000
